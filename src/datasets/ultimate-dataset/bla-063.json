{
    "text": "Artificial neural networks have become widely recognized as viable computational models for simulating human language processing. One of the main criticisms leveled against these models is their extensive reliance on training data, surpassing the amount of data humans typically encounter during language acquisition. This study adopts two complementary methodologies to investigate how varying amounts of training data affect the ability of GPT-2 models to capture human fMRI responses to sentences. Initially, we assess GPT-2 models trained on datasets containing 1 million, 10 million, 100 million, or 1 billion words against an fMRI benchmark. The 100-million-word model is deemed developmentally plausible, reflecting a similar scale of exposure to what children experience in the first decade of life. Additionally, we examine the performance of a GPT-2 model trained on a 9-billion-token dataset to achieve state-of-the-art next-word prediction on the human benchmark at different training stages. Our findings indicate that models trained on a developmentally plausible amount of data exhibit nearly optimal performance in capturing fMRI responses to sentences. Moreover, we observe that lower perplexity, a measure of next-word prediction performance, correlates with stronger alignment with human data. This suggests that models achieving high next-word prediction performance also acquire representations of sentences predictive of human fMRI responses. These results suggest that while training is essential for predictive ability, a developmentally realistic amount of training (~100 million words) may be adequate.",
    "label": 1,
    "type": 5,
    "name": "bla-063"
}