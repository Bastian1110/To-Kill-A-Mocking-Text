{
    "text": "Artificial neural networks are increasingly utilized as computational models for emulating human language processing. One of the primary criticisms directed at these models is their heavy reliance on training data, surpassing the amount of data encountered by humans during language learning. This study employs two complementary methodologies to investigate how varying training data sizes impact GPT-2 models' ability to capture human fMRI responses to sentences. Initially, we assess GPT-2 models trained on datasets comprising 1 million, 10 million, 100 million, or 1 billion words against an fMRI benchmark. The 100-million-word model is considered developmentally plausible, akin to the amount of exposure children receive in their early years. Additionally, we evaluate the performance of a GPT-2 model trained on a 9-billion-token dataset to achieve state-of-the-art next-word prediction on the human benchmark at different training stages. Our findings indicate that models trained on a developmentally plausible amount of data achieve near-maximal performance in capturing fMRI responses to sentences. Furthermore, we observe a correlation between lower perplexity (a measure of next-word prediction performance) and stronger alignment with human data, suggesting that models achieving high next-word prediction performance also develop representations of sentences predictive of human fMRI responses. These results imply that while training is crucial for predictive ability, a developmentally realistic amount of training (~100 million words) may be sufficient.",
    "label": 0,
    "type": 7,
    "name": "new-063"
}