{
    "text": "Implementing empathy into healthcare chatbots is seen as a promising approach to imbue them with a sense of human warmth. However, current research often overlooks the multifaceted nature of empathy, leading to a limited understanding of whether artificial empathy is perceived similarly to interpersonal empathy. This study argues that incorporating experiential empathy expressions could have unintended negative consequences, potentially feeling inauthentic. Instead, providing instrumental support may be more effective in modeling artificial empathy, aligning better with computer-like structures in chatbots. Two experimental studies using healthcare chatbots investigate the impact of empathetic (feeling with), sympathetic (feeling for), and behavioral-empathetic (empathetic helping) versus non-empathetic responses on perceived warmth, authenticity, trust, and usage intentions. The findings indicate that any form of empathy (compared to no empathy) increases perceived warmth, leading to greater trust and usage intentions. As predicted, empathetic and sympathetic responses diminish the chatbot's perceived authenticity, countering the positive impact observed in both studies. Interestingly, a third study involving human-human interactions does not replicate this negative effect. This research emphasizes the disparity in applying empathy to human-bot interactions and introduces the concept of 'perceived authenticity,' highlighting that human-like attributes can sometimes backfire when interacting with chatbots.",
    "label": 1,
    "type": 5,
    "name": "bla-050"
}