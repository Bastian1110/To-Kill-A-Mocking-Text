Incorporating empathy into healthcare chatbots holds promise for instilling a sense of human warmth. However, existing research often overlooks the multidimensional nature of empathy, leading to a limited understanding of whether artificial empathy is perceived similarly to interpersonal empathy. This study argues that implementing experiential expressions of empathy may have unintended negative consequences, as they may come across as inauthentic. Instead, providing instrumental support could be more suitable for modeling artificial empathy, as it aligns better with computer-like schemas toward chatbots. Two experimental studies utilizing healthcare chatbots investigate the impact of empathetic (feeling with), sympathetic (feeling for), and behavioral-empathetic (empathetic helping) versus non-empathetic responses on perceived warmth, perceived authenticity, and their effects on trust and usage intentions. The results indicate that any form of empathy (compared to no empathy) enhances perceived warmth, leading to increased trust and usage intentions. As predicted, empathetic and sympathetic responses diminish the chatbot's perceived authenticity, counteracting this positive effect in both studies. A third study fails to replicate this adverse effect in human-human interactions. This research underscores that empathy does not translate equally in human-bot interactions and introduces the concept of 'perceived authenticity,' highlighting that distinctly human attributes may backfire when interacting with chatbots by seeming inauthentic.