The Multimodal Emotion Recognition in Conversations (ERC) task focuses on identifying emotions conveyed in each utterance within a conversational video. However, existing approaches encounter challenges in balancing intra- and inter-speaker context dependencies, especially in handling intra-modal interactions. This balance is crucial as it involves modeling emotional inertia (self-dependency), where a speaker's own emotions impact them, and modeling empathy (interpersonal dependency), where the emotions of counterparts influence a speaker. Additionally, addressing cross-modal interactions involving content with conflicting emotions across different modalities poses challenges. To tackle these issues, we propose an adaptive interactive graph network (IGN) called AdaIGN, which leverages the Gumbel Softmax trick to adaptively select nodes and edges, thereby enhancing intra- and cross-modal interactions. Unlike undirected graphs, we employ a directed IGN to prevent future utterances from influencing the current one. Furthermore, we introduce Node- and Edge-level Selection Policies (NESP) to guide node and edge selection, along with a Graph-Level Selection Policy (GSP) to integrate utterance representations from the original IGN and NESP-enhanced IGN. Additionally, we design a task-specific loss function that prioritizes text modality and intra-speaker context selection. To reduce computational complexity, we utilize pre-defined pseudo labels via self-supervised methods to mask unnecessary utterance nodes for selection. Experimental results demonstrate that AdaIGN surpasses state-of-the-art methods on two widely-used datasets